{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Acey Deucey using Reinforcement Learning\n",
    "[Michael DiSanto](https://www.michaelpdisanto.com) - 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Objective\n",
    "The objective of this project is to design and implement a reinforcement learning-based optimal betting strategy for the card game \"Acey-Deucey\" (also known as \"In Between\"). Through the use of object-oriented programming in Python, we aim to create an intelligent agent that can learn and adapt its betting decisions during gameplay. This agent will consider factors such as the current state of the game, the player's hand, and the pot's size to make informed betting choices. By developing this AI player, I seek to optimize the player's betting decisions, ultimately increasing their chances of success and accumulating the most chips by the end of the game. This project will explore the intersection of game theory, machine learning, and strategic decision-making to achieve an effective and competitive Acey Deucey player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acey Deucey Description\n",
    "In-Between is not very popular at casinos, but is often played in home Poker games as a break from Poker itself. The rules below are for the home game, which is easily adaptable for casino play.\n",
    "\n",
    "### Rank of Cards\n",
    "A (high), K, Q, J, 10, 9, 8, 7, 6, 5, 4, 3, 2.\n",
    "\n",
    "### Object of the Game\n",
    "The goal is to be the player with the most chips at the end of the game.\n",
    "\n",
    "### The Ante\n",
    "Chips are distributed to the players, and each players puts one chip in the center of the table to form a pool or pot.\n",
    "\n",
    "### The Draw\n",
    "Any player deals one card face up, to each player in turn, and the player with the highest card deals first.\n",
    "\n",
    "### The Shuffle, Cut, and Deal\n",
    "Any player may shuffle, and the dealer shuffles last. The player to the dealer's right cuts the cards. The dealer turns up two cards and places them in the middle of the table, positioning them so that there is ample room for a third card to fit in between.\n",
    "\n",
    "### The Betting\n",
    "The player on the dealer's left may bet up to the entire pot or any portion of the number of chips in the pot, but they must always bet a minimum of one chip. When the player has placed a bet, the dealer turns up the top card from the pack and places it between the two cards already face up. If the card ranks between the two cards already face up, the player wins and takes back the amount of his bet plus an equivalent amount from the pot. If the third card is not between the face-up cards, or is of the same rank as either of them, the player loses his bet, and it is added to the pot. If the two face-up cards up are consecutive, the player automatically loses, and a third card need not be turned up. If the two face-up cards are the same, the player wins two chips and, again, no third card is turned up. (In some games, the player is paid three chips when this occurs.)\n",
    "\n",
    "\"Acey-Deucey\" (ace, 2) is the best combination, and a player tends to bet the whole pot, if they can. This is because the only way an ace-deuce combination can lose is if the third card turned up is also an ace or a deuce.\n",
    "\n",
    "After the first player has finished, the dealer clears away the cards and places them face down in a pile. The next player then places a bet, and the dealer repeats the same procedure until all the players, including the dealer, have had a turn.\n",
    "\n",
    "If at any time, the pot has no more chips in it (because a player has \"bet the pot\" and won), each player again puts in one chip to restore the pot.\n",
    "\n",
    "When every player has had a turn to bet, the deal passes to the player on the dealer's left, and the game continues.\n",
    "\n",
    "https://bicyclecards.com/how-to-play/in-between/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self):\n",
    "        self.hand = [random.randint(2, 14) for _ in range(2)]\n",
    "    \n",
    "    def draw_card(self):\n",
    "        return random.randint(2, 14)\n",
    "\n",
    "    def make_bet(self, current_pot):\n",
    "        # Implement your betting strategy here\n",
    "        # You can use RL model to decide the bet amount\n",
    "        return random.randint(1, current_pot)  # A simple random betting strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acey Deucey Game Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, num_players):\n",
    "        self.num_players = num_players\n",
    "        self.players = [Player() for _ in range(num_players)]\n",
    "        self.pot = 0\n",
    "\n",
    "    def deal_initial_cards(self):\n",
    "        # Simulate dealing one card to each player to determine the dealer\n",
    "        dealer = max(self.players, key=lambda p: p.draw_card())\n",
    "        self.players.remove(dealer)\n",
    "        self.players.insert(0, dealer)\n",
    "\n",
    "    def play_round(self):\n",
    "        for player in self.players:\n",
    "            print(f\"Player {self.players.index(player) + 1}'s turn\")\n",
    "            bet = player.make_bet(self.pot)\n",
    "            self.pot += bet\n",
    "            player.hand.append(player.draw_card())\n",
    "            print(f\"Player {self.players.index(player) + 1} drew a card: {player.hand[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(num_episodes, learning_rate, discount_factor, epsilon):\n",
    "    # Q-learning algorithm\n",
    "    Q = np.zeros((15, 2))  # Q-table for state-action pairs\n",
    "    for episode in range(num_episodes):\n",
    "        game = Game(num_players=2)  # Two players\n",
    "        game.deal_initial_cards()\n",
    "        for _ in range(2):  # Two rounds per episode\n",
    "            for state in range(2, 15):\n",
    "                for action in range(2):\n",
    "                    if random.uniform(0, 1) < epsilon:\n",
    "                        next_action = random.randint(0, 1)\n",
    "                    else:\n",
    "                        next_action = np.argmax(Q[state, :])\n",
    "                    next_state = min(state + (2 * random.randint(2, 14)), 14)  # Simulate drawing a card\n",
    "                    reward = 0  # Implement your reward function here\n",
    "                    Q[state, action] = (1 - learning_rate) * Q[state, action] + learning_rate * (reward + discount_factor * Q[next_state, next_action])\n",
    "        epsilon *= 0.99  # Decay epsilon\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Game (1000 Iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.1\n",
    "Q = q_learning(num_episodes, learning_rate, discount_factor, epsilon)\n",
    "print(\"Q-table:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is not working..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
